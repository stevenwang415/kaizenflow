<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Unit tests - My Docs</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href=".." class="nav-link">Welcome to MkDocs</a>
                            </li>
                            <li class="navitem">
                                <a href="../Buildmeister_process/" class="nav-link">Buildmeister process</a>
                            </li>
                            <li class="navitem">
                                <a href="../Code_review/" class="nav-link">Code review</a>
                            </li>
                            <li class="navitem">
                                <a href="../Codebase_clean_up_scripts/" class="nav-link">Codebase clean-up scripts</a>
                            </li>
                            <li class="navitem">
                                <a href="../Coding_Style_Guide/" class="nav-link">Sorrentum - Python Style Guide</a>
                            </li>
                            <li class="navitem">
                                <a href="../Contributor_Scorecard/" class="nav-link">Contributor Scorecard</a>
                            </li>
                            <li class="navitem">
                                <a href="../DataFlow/" class="nav-link">DataFlow specification</a>
                            </li>
                            <li class="navitem">
                                <a href="../DataPull/" class="nav-link">DataPull</a>
                            </li>
                            <li class="navitem">
                                <a href="../Design_Philosophy/" class="nav-link">Design Philosophy</a>
                            </li>
                            <li class="navitem">
                                <a href="../Development_workflow/" class="nav-link">Development Workflow</a>
                            </li>
                            <li class="navitem">
                                <a href="../Docker/" class="nav-link">Docker</a>
                            </li>
                            <li class="navitem">
                                <a href="../Documentation_about_guidelines/" class="nav-link">Documentation about guidelines</a>
                            </li>
                            <li class="navitem">
                                <a href="../Email/" class="nav-link">Email</a>
                            </li>
                            <li class="navitem">
                                <a href="../First_review_process/" class="nav-link">First Review Process</a>
                            </li>
                            <li class="navitem">
                                <a href="../From_zero_to_modeling/" class="nav-link">From zero to modeling</a>
                            </li>
                            <li class="navitem">
                                <a href="../General_rules_of_collaboration/" class="nav-link">General Rules of Collaboration</a>
                            </li>
                            <li class="navitem">
                                <a href="../GitHub_ZenHub_workflows/" class="nav-link">GitHub/ZenHub workflows</a>
                            </li>
                            <li class="navitem">
                                <a href="../Git_workflow_and_best_practices/" class="nav-link">Git workflow and best practices</a>
                            </li>
                            <li class="navitem">
                                <a href="../Glossary/" class="nav-link">Glossary</a>
                            </li>
                            <li class="navitem">
                                <a href="../Gsheet_into_pandas/" class="nav-link">Gsheet into pandas</a>
                            </li>
                            <li class="navitem">
                                <a href="../HTMLcov_server/" class="nav-link">HTMLcov server</a>
                            </li>
                            <li class="navitem">
                                <a href="../How_to_integrate_repos/" class="nav-link">How to integrate repos</a>
                            </li>
                            <li class="navitem">
                                <a href="../How_to_organize_your_work/" class="nav-link">Golden rules</a>
                            </li>
                            <li class="navitem">
                                <a href="../Imports_and_packages/" class="nav-link">Imports and packages</a>
                            </li>
                            <li class="navitem">
                                <a href="../Jupyter_notebook_best_practices/" class="nav-link">Jupyter notebook best practices</a>
                            </li>
                            <li class="navitem">
                                <a href="../Reading_List/" class="nav-link">Reading List</a>
                            </li>
                            <li class="navitem">
                                <a href="../Receive_crypto_payment/" class="nav-link">Receive crypto payment</a>
                            </li>
                            <li class="navitem">
                                <a href="../Scrum_methodology/" class="nav-link">Scrum Methodology</a>
                            </li>
                            <li class="navitem">
                                <a href="../Signing_up_for_Sorrentum/" class="nav-link">Signing up to the project</a>
                            </li>
                            <li class="navitem">
                                <a href="../Sorrentum_development_setup/" class="nav-link">Sorrentum development setup</a>
                            </li>
                            <li class="navitem">
                                <a href="../Telegram/" class="nav-link">Telegram</a>
                            </li>
                            <li class="navitem">
                                <a href="../Tools-PyCharm/" class="nav-link">Running PyCharm remotely</a>
                            </li>
                            <li class="navitem">
                                <a href="../Type_hints/" class="nav-link">Type hints</a>
                            </li>
                            <li class="navitem active">
                                <a href="./" class="nav-link">Unit tests</a>
                            </li>
                            <li class="navitem">
                                <a href="../Visual_Studio_Code/" class="nav-link">Visual Studio Code</a>
                            </li>
                            <li class="navitem">
                                <a href="../Workflows/" class="nav-link">Navigate stack of failed test</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../Type_hints/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../Visual_Studio_Code/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#unit-tests" class="nav-link">Unit tests</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#running-unit-tests" class="nav-link">Running unit tests</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#using-invoke" class="nav-link">Using invoke</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#timeout" class="nav-link">Timeout</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#rerunning-timeout-ed-tests" class="nav-link">Rerunning timeout-ed tests</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#compute-tests-coverage_1" class="nav-link">Compute tests coverage</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#running-pytest-directly" class="nav-link">Running pytest directly</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#usage-and-invocations-reference" class="nav-link">Usage and Invocations reference</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#custom-pytest-options-behaviors" class="nav-link">Custom pytest options behaviors</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#debugging-notebooks" class="nav-link">Debugging Notebooks</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#running-tests-on-gh-actions" class="nav-link">Running tests on GH Actions</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#how-to-run-a-single-test-on-gh-action" class="nav-link">How to run a single test on GH Action</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#guidelines-about-writing-unit-tests" class="nav-link">Guidelines about writing unit tests</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#unit-testing-tips" class="nav-link">Unit testing tips</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#conventions" class="nav-link">Conventions</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#update-test-tags" class="nav-link">Update test tags</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#mocking" class="nav-link">Mocking</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#refs" class="nav-link">Refs</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#common-usage-samples" class="nav-link">Common usage samples</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#philosophy-about-mocking" class="nav-link">Philosophy about mocking</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#some-general-suggestions-about-testing" class="nav-link">Some general suggestions about testing</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#object-patch-with-return-value" class="nav-link">Object patch with return value</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#path-patch-with-multiple-return-values" class="nav-link">Path patch with multiple return values</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#ways-of-calling-patch-and-patchobject" class="nav-link">Ways of calling patch and patch.object</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#mock-object-state-after-test-run" class="nav-link">Mock object state after test run</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#mock-common-external-calls-in-hunitesttestcase-class" class="nav-link">Mock common external calls in hunitest.TestCase class</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#mocks-with-specs" class="nav-link">Mocks with specs</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#caveats" class="nav-link">Caveats</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="unit-tests">Unit tests</h1>
<!-- toc -->

<ul>
<li><a href="#running-unit-tests">Running unit tests</a></li>
<li><a href="#using-invoke">Using <code>invoke</code></a><ul>
<li><a href="#docker-image-stage-and-version">Docker image stage and version</a></li>
<li><a href="#specifying-pytest-options">Specifying <code>pytest</code> options</a></li>
<li><a href="#save-test-output-to-file">Save test output to file</a></li>
<li><a href="#show-the-tests-but-do-not-run">Show the tests but do not run</a></li>
<li><a href="#skip-submodules">Skip submodules</a></li>
<li><a href="#compute-tests-coverage">Compute tests coverage</a></li>
</ul>
</li>
<li><a href="#timeout">Timeout</a></li>
<li><a href="#rerunning-timeout-ed-tests">Rerunning timeout-ed tests</a></li>
<li><a href="#compute-tests-coverage-1">Compute tests coverage</a><ul>
<li><a href="#an-example-coverage-session">An example coverage session</a></li>
<li><a href="#an-example-with-customized-pytest-cov-html-run">An example with customized <code>pytest-cov</code> html run</a></li>
<li><a href="#generate-coverage-report-with-invoke">Generate coverage report with <code>invoke</code></a></li>
<li><a href="#common-usage">Common usage</a></li>
<li><a href="#publishing-html-report-on-s3">Publishing HTML report on S3</a></li>
</ul>
</li>
<li><a href="#running-pytest-directly">Running <code>pytest</code> directly</a></li>
<li><a href="#usage-and-invocations-reference">Usage and Invocations reference</a></li>
<li><a href="#custom-pytest-options-behaviors">Custom <code>pytest</code> options behaviors</a><ul>
<li><a href="#enable-logging">Enable logging</a></li>
<li><a href="#update-golden-outcomes">Update golden outcomes</a></li>
<li><a href="#incremental-test-mode">Incremental test mode</a></li>
</ul>
</li>
<li><a href="#debugging-notebooks">Debugging Notebooks</a></li>
<li><a href="#running-tests-on-gh-actions">Running tests on GH Actions</a></li>
<li><a href="#how-to-run-a-single-test-on-gh-action">How to run a single test on GH Action</a></li>
<li><a href="#guidelines-about-writing-unit-tests">Guidelines about writing unit tests</a><ul>
<li><a href="#what-is-a-unit-test"><strong>What is a unit test?</strong></a></li>
<li><a href="#why-is-unit-testing-important"><strong>Why is unit testing important?</strong></a></li>
</ul>
</li>
<li><a href="#unit-testing-tips">Unit testing tips</a><ul>
<li><a href="#tip-%231-test-one-thing">Tip #1: Test one thing</a></li>
<li><a href="#tip-%232-keep-tests-self-contained">Tip #2: Keep tests self-contained</a></li>
<li><a href="#tip-%233-only-specify-data-related-to-what-is-being-tested">Tip #3: Only specify data related to what is being tested</a></li>
<li><a href="#tip-%234-test-realistic-corner-cases">Tip #4: Test realistic corner cases</a></li>
<li><a href="#tip-%235-test-a-typical-scenario">Tip #5: Test a typical scenario</a></li>
<li><a href="#tip-%236-test-executable-scripts-end-to-end">Tip #6: Test executable scripts end-to-end</a></li>
</ul>
</li>
<li><a href="#conventions">Conventions</a><ul>
<li><a href="#naming-and-placement-conventions">Naming and placement conventions</a></li>
<li><a href="#our-framework-to-test-using-input--output-data">Our framework to test using input / output data</a></li>
<li><a href="#use-text-and-not-pickle-files-as-input">Use text and not pickle files as input</a></li>
<li><a href="#check_string-vs-selfassertequal"><code>check_string</code> vs <code>self.assertEqual</code></a></li>
<li><a href="#use-selfassert_equal">Use <code>self.assert_equal</code></a></li>
<li><a href="#how-to-split-unit-test-code-in-files">How to split unit test code in files</a></li>
<li><a href="#skeleton-for-unit-test">Skeleton for unit test</a></li>
<li><a href="#hierarchical-testcase-approach">Hierarchical <code>TestCase</code> approach</a></li>
<li><a href="#use-the-appropriate-selfassert">Use the appropriate <code>self.assert*</code></a></li>
<li><a href="#do-not-use-hdbgdassert">Do not use <code>hdbg.dassert</code></a></li>
<li><a href="#interesting-testing-functions">Interesting testing functions</a></li>
<li><a href="#use-setupteardown">Use setUp/tearDown</a></li>
</ul>
</li>
<li><a href="#update-test-tags">Update test tags</a></li>
<li><a href="#mocking">Mocking</a></li>
<li><a href="#refs">Refs</a></li>
<li><a href="#common-usage-samples">Common usage samples</a></li>
<li><a href="#philosophy-about-mocking">Philosophy about mocking</a></li>
<li><a href="#some-general-suggestions-about-testing">Some general suggestions about testing</a><ul>
<li><a href="#test-from-the-outside-in">Test from the outside-in</a></li>
<li><a href="#we-dont-need-to-test-all-the-assertions">We don't need to test all the assertions</a></li>
<li><a href="#use-strings-to-compare-output-instead-of-data-structures">Use strings to compare output instead of data structures</a></li>
<li><a href="#use-selfcheck_string-for-things-that-we-care-about-not-changing-or-are-too-big-to-have-as-strings-in-the-code">Use <code>self.check_string()</code> for things that we care about not changing (or are too big to have as strings in the code)</a></li>
<li><a href="#each-test-method-should-test-a-single-test-case">Each test method should test a single test case</a></li>
<li><a href="#each-test-should-be-crystal-clear-on-how-it-is-different-from-the-others">Each test should be crystal clear on how it is different from the others</a></li>
<li><a href="#in-general-you-want-to-budget-the-time-to-write-unit-tests">In general you want to budget the time to write unit tests</a></li>
<li><a href="#write-skeleton-of-unit-tests-and-ask-for-a-review-if-you-are-not-sure-how-what-to-test">Write skeleton of unit tests and ask for a review if you are not sure how what to test</a></li>
</ul>
</li>
<li><a href="#object-patch-with-return-value">Object patch with return value</a></li>
<li><a href="#path-patch-with-multiple-return-values">Path patch with multiple return values</a></li>
<li><a href="#ways-of-calling-patch-and-patchobject">Ways of calling <code>patch</code> and <code>patch.object</code></a></li>
<li><a href="#mock-object-state-after-test-run">Mock object state after test run</a></li>
<li><a href="#mock-common-external-calls-in-hunitesttestcase-class">Mock common external calls in <code>hunitest.TestCase</code> class</a></li>
<li><a href="#mocks-with-specs">Mocks with specs</a></li>
<li><a href="#caveats">Caveats</a></li>
</ul>
<!-- tocstop -->

<h1 id="running-unit-tests">Running unit tests</h1>
<ul>
<li>Before any PR (and ideally after every commit) we want to run all the unit
  tests to make sure we didn't introduce no new bugs</li>
<li>We use <code>pytest</code> and <code>unittest</code> as testing framework</li>
<li>We have different test set lists:</li>
<li><code>fast</code><ul>
<li>Tests that are quick to execute (typically &lt; 5 secs per test class)</li>
<li>We want to run these tests before / after every commit / PR to make sure
  things are not horrible broken</li>
</ul>
</li>
<li><code>slow</code><ul>
<li>Tests that we don't want to run all the times because they are:</li>
<li>Slow (typically &lt; 20 seconds per test)</li>
<li>Related to pieces of code that don't change often</li>
<li>External APIs we don't want to hit continuously</li>
</ul>
</li>
<li><code>superslow</code><ul>
<li>Tests that run long workload, e.g., running a production model</li>
</ul>
</li>
</ul>
<h2 id="using-invoke">Using <code>invoke</code></h2>
<ul>
<li><a href="https://www.pyinvoke.org/"><code>invoke</code></a> is a task execution which allows to
  execute some typical workflows, e.g. run the tests</li>
</ul>
<p>```bash
  # Run only fast tests:</p>
<blockquote>
<p>i run_fast_tests
  # Run only slow tests:
i run_slow_tests
  # Run only superslow tests:
i run_superslow_tests</p>
</blockquote>
<p>To see the options use <code>--help</code> option, e.g. <code>i --help run_fast_tests</code>:</p>
<p>Usage: inv[oke] [--core-opts] run_fast_tests [--options] [other tasks here ...]</p>
<p>Docstring:
    Run fast tests.
    :param stage: select a specific stage for the Docker image
    :param pytest_opts: option for pytest
    :param skip_submodules: ignore all the dir inside a submodule
    :param coverage: enable coverage computation
    :param collect_only: do not run tests but show what will be executed
    :param tee_to_file: save output of pytest in <code>tmp.pytest.log</code>
    :param kwargs: kwargs for <code>ctx.run</code></p>
<p>Options:
    -c, --coverage
    -k, --skip-submodules
    -o, --collect-only
    -p STRING, --pytest-opts=STRING
    -s STRING, --stage=STRING
    -t, --tee-to-file
    -v STRING, --version=STRING
  ```</p>
<h3 id="docker-image-stage-and-version">Docker image stage and version</h3>
<ul>
<li>To select a specific stage for Docker image use the <code>--stage</code> option. E.g.,
  this might be useful when a user wants to run regressions on the local Docker
  image to verify that nothing is broken before promoting it to <code>dev</code> image.</li>
</ul>
<p>```bash</p>
<blockquote>
<p>i run_fast_tests --stage local
  ```</p>
</blockquote>
<ul>
<li>To run the tests on the specific version of a Docker image, use the
  <code>--version</code> option. E.g., this might be useful when releasing a new version of
  an image.</li>
</ul>
<p>```bash</p>
<blockquote>
<p>i run_fast_tests --stage local --version 1.0.4
  ```</p>
</blockquote>
<h3 id="specifying-pytest-options">Specifying <code>pytest</code> options</h3>
<ul>
<li>With the option <code>--pytest-opts</code> it is possible to pass any <code>pytest</code> option to
  <code>invoke</code>. E.g., if a user want to run the tests in the debug mode to show the
  output</li>
</ul>
<p>```bash</p>
<blockquote>
<p>i run_fast_tests -s --dbg
  ```</p>
</blockquote>
<h3 id="save-test-output-to-file">Save test output to file</h3>
<ul>
<li>To save the output of <code>pytest</code> to <code>tmp.pytest.log</code> use the <code>--tee-to-file</code>
  option.</li>
</ul>
<p>```bash</p>
<blockquote>
<p>i run_fast_tests --tee-to-file
  ```</p>
</blockquote>
<h3 id="show-the-tests-but-do-not-run">Show the tests but do not run</h3>
<ul>
<li>To show (but not run) the tests that will be executed, use the
  <code>--collect-only</code>.</li>
</ul>
<p>```bash</p>
<blockquote>
<p>i run_fast_test --collect-only
  ```</p>
</blockquote>
<h3 id="skip-submodules">Skip submodules</h3>
<ul>
<li>To skip running tests in submodules use the <code>--skip-submodules</code> option. This
  is useful for repos with submodules, e.g., <code>dev_tools</code> where <code>cmamp</code> is a
  submodule. Using this option, only tests in <code>dev_tools</code> but not in <code>cmamp</code> are
  run</li>
</ul>
<p>```bash</p>
<blockquote>
<p>cd dev_tools1
i run_fast_tests --skip-submodules
  ```</p>
</blockquote>
<h3 id="compute-tests-coverage">Compute tests coverage</h3>
<ul>
<li>To compute tests coverage use the <code>--coverage</code> option</li>
</ul>
<p>```bash</p>
<blockquote>
<p>i run_fast_tests --coverage
  ```</p>
</blockquote>
<h2 id="timeout">Timeout</h2>
<ul>
<li>We use the <a href="https://pypi.org/project/pytest-timeout/"><code>pytest-timeout</code></a>
  package to limit durations of fast, slow, and superslow tests</li>
<li>The timeout durations for each test type are listed
  <a href="#running-unit-tests">here</a></li>
<li>The timeout restricts only running time of the test method, not <code>setUp</code> and
  <code>tearDown</code> time</li>
</ul>
<h2 id="rerunning-timeout-ed-tests">Rerunning timeout-ed tests</h2>
<ul>
<li>Running tests can take different amount of time depending on workload and
  machine</li>
<li>Because of this, we rerun failing tests using
  <a href="https://pypi.org/project/pytest-rerunfailures/"><code>pytest-rerunfailures</code></a></li>
<li><code>pytest-rerunfailures</code> is not completely compatible with <code>pytest-timeout</code>.
  This is why we have to add the <code>-o timeout_func_only=true</code> flag to
  <code>pytest-timeout</code>. See
  <a href="https://github.com/pytest-dev/pytest-rerunfailures/issues/99">https://github.com/pytest-dev/pytest-rerunfailures/issues/99</a><code></code>for
  more information</li>
<li>We rerun timeouted fast tests twice and timeouted slow and superslow tests
  once</li>
<li>There is a
  <a href="https://pypi.org/project/pytest-rerunfailures/#re-run-individual-failures">way</a>
  to provide a rerun delay for individual tests. However, we canâ€™t use it for
  now due to
  <a href="https://github.com/cryptokaizen/cmamp/issues/693#issuecomment-989456031">#693 (comment)</a></li>
</ul>
<h2 id="compute-tests-coverage_1">Compute tests coverage</h2>
<ul>
<li>
<p>The documentation for <code>coverage</code> is
  <a href="https://coverage.readthedocs.io/en/latest/cmd.html#reporting">here</a>.</p>
</li>
<li>
<p>Run a set of unit tests enabling coverage:</p>
</li>
</ul>
<p>```bash
  # Run the coverage for a single test:</p>
<blockquote>
<p>i run_fast_tests --coverage -p oms/test/test_broker.py::TestSimulatedBroker1</p>
</blockquote>
<p># Run coverage for an entire module like <code>oms</code>:</p>
<blockquote>
<p>i run_fast_tests --coverage -p oms</p>
</blockquote>
<p>```</p>
<ul>
<li>This generates and run a pytest command inside Docker like:</li>
</ul>
<p><code>bash
  docker&gt; /venv/bin/pytest -m "not slow and not superslow" oms/test/test_broker.py::TestSimulatedBroker1 --cov=. --cov-branch --cov-report term-missing --cov-report html</code></p>
<ul>
<li>
<p>Which generates:</p>
</li>
<li>
<p>A default coverage report</p>
</li>
<li>A binary <code>.coverage</code> file that contains the coverage information</li>
<li>
<p>An <code>htmlcov</code> dir with a browsable code output to inspect the coverage for
    the files</p>
</li>
<li>
<p>One can post-process the coverage report in different ways using the command
  <code>coverage</code> inside a docker container, since the code was run (as always)
  inside the Docker container that contains all the dependencies.</p>
</li>
</ul>
<p>```bash</p>
<blockquote>
<p>coverage -h</p>
</blockquote>
<p>Coverage.py, version 5.5 with C extension
  Measure, collect, and report on code coverage in Python programs.</p>
<p>usage: coverage <command> [options] [args]</p>
<p>Commands:
      annotate    Annotate source files with execution information.
      combine     Combine a number of data files.
      debug       Display information about the internals of coverage.py
      erase       Erase previously collected coverage data.
      help        Get help on using coverage.py.
      html        Create an HTML report.
      json        Create a JSON report of coverage results.
      report      Report coverage stats on modules.
      run         Run a Python program and measure code execution.
      xml         Create an XML report of coverage results.</p>
<p>Use "coverage help <command>" for detailed help on any command.
  Full documentation is at https://coverage.readthedocs.io
  ```</p>
<p>```bash</p>
<blockquote>
<p>coverage report -h</p>
</blockquote>
<p>Usage: coverage report [options] [modules]</p>
<p>Report coverage statistics on modules.</p>
<p>Options:
    --contexts=REGEX1,REGEX2,...
                          Only display data from lines covered in the given
                          contexts. Accepts Python regexes, which must be
                          quoted.
    --fail-under=MIN      Exit with a status of 2 if the total coverage is less
                          than MIN.
    -i, --ignore-errors   Ignore errors while reading source files.
    --include=PAT1,PAT2,...
                          Include only files whose paths match one of these
                          patterns. Accepts shell-style wildcards, which must be
                          quoted.
    --omit=PAT1,PAT2,...  Omit files whose paths match one of these patterns.
                          Accepts shell-style wildcards, which must be quoted.
    --precision=N         Number of digits after the decimal point to display
                          for reported coverage percentages.
    --sort=COLUMN         Sort the report by the named column: name, stmts,
                          miss, branch, brpart, or cover. Default is name.
    -m, --show-missing    Show line numbers of statements in each module that
                          weren't executed.
    --skip-covered        Skip files with 100% coverage.
    --no-skip-covered     Disable --skip-covered.
    --skip-empty          Skip files with no code.
    --debug=OPTS          Debug options, separated by commas. [env:
                          COVERAGE_DEBUG]
    -h, --help            Get help on this command.
    --rcfile=RCFILE       Specify configuration file. By default '.coveragerc',
                          'setup.cfg', 'tox.ini', and 'pyproject.toml' are
                          tried. [env: COVERAGE_RCFILE]
  ```</p>
<p>```bash</p>
<blockquote>
<p>i docker_bash</p>
</blockquote>
<p># Report the coverage for all the files under <code>oms</code> using the workload above (i.e., the fast tests under <code>oms/test/test_broker.py::TestSimulatedBroker1</code>)
  docker&gt; coverage report --include="oms/*"</p>
<p>Name                                    Stmts   Miss Branch BrPart  Cover</p>
<hr />
<p>oms/<strong>init</strong>.py                             0      0      0      0   100%
  oms/api.py                                154     47     36      2    70%
  oms/broker.py                             200     31     50      9    81%
  oms/broker_example.py                      23      0      4      1    96%
  oms/call_optimizer.py                      31      0      0      0   100%
  oms/devops/<strong>init</strong>.py                      0      0      0      0   100%
  oms/devops/docker_scripts/<strong>init</strong>.py       0      0      0      0   100%
  oms/locates.py                              7      7      2      0     0%
  oms/mr_market.py                           55      1     10      1    97%
  oms/oms_db.py                              47      0     10      3    95%
  oms/oms_lib_tasks.py                       64     39      2      0    38%
  oms/oms_utils.py                           34     34      6      0     0%
  oms/order.py                              101     30     22      0    64%
  oms/order_example.py                       26      0      0      0   100%
  oms/place_orders.py                       121      8     18      6    90%
  oms/pnl_simulator.py                      326     42     68      8    83%
  oms/portfolio.py                          309     21     22      0    92%
  oms/portfolio_example.py                   32      0      0      0   100%
  oms/tasks.py                                3      3      0      0     0%
  oms/test/oms_db_helper.py                  29     11      2      0    65%
  oms/test/test_api.py                      132     25     12      0    83%
  oms/test/test_broker.py                    33      5      4      0    86%
  oms/test/test_mocked_portfolio.py           0      0      0      0   100%
  oms/test/test_mr_market.py                 46      0      2      0   100%
  oms/test/test_oms_db.py                   114     75     14      0    38%
  oms/test/test_order.py                     24      0      4      0   100%
  oms/test/test_place_orders.py              77      0      4      0   100%
  oms/test/test_pnl_simulator.py            235      6     16      0    98%
  oms/test/test_portfolio.py                135      0      6      0   100%</p>
<hr />
<p>TOTAL                                    2358    385    314     30    82%
  ```</p>
<ul>
<li>To exclude the test files, which could inflate the coverage</li>
</ul>
<p>```bash</p>
<blockquote>
<p>coverage report --include="oms/<em>" --omit="</em>/test_*.py"</p>
</blockquote>
<p>Name                                    Stmts   Miss Branch BrPart  Cover</p>
<hr />
<p>oms/<strong>init</strong>.py                             0      0      0      0   100%
  oms/api.py                                154     47     36      2    70%
  oms/broker.py                             200     31     50      9    81%
  oms/broker_example.py                      23      0      4      1    96%
  oms/call_optimizer.py                      31      0      0      0   100%
  oms/devops/<strong>init</strong>.py                      0      0      0      0   100%
  oms/devops/docker_scripts/<strong>init</strong>.py       0      0      0      0   100%
  oms/locates.py                              7      7      2      0     0%
  oms/mr_market.py                           55      1     10      1    97%
  oms/oms_db.py                              47      0     10      3    95%
  oms/oms_lib_tasks.py                       64     39      2      0    38%
  oms/oms_utils.py                           34     34      6      0     0%
  oms/order.py                              101     30     22      0    64%
  oms/order_example.py                       26      0      0      0   100%
  oms/place_orders.py                       121      8     18      6    90%
  oms/pnl_simulator.py                      326     42     68      8    83%
  oms/portfolio.py                          309     21     22      0    92%
  oms/portfolio_example.py                   32      0      0      0   100%
  oms/tasks.py                                3      3      0      0     0%
  oms/test/oms_db_helper.py                  29     11      2      0    65%</p>
<hr />
<p>TOTAL                                    1562    274    252     30    80%
  ```</p>
<ul>
<li>To open the line coverage, from outside Docker go with your browser to
  <code>htmlcov/index.html</code>. The <code>htmlcov</code> is re-written with every coverage run with
  the <code>--cov-report html</code> option. If you move out <code>index.html</code> from <code>htmlcov</code>
  dir some html features (e.g., filtering) will not work.</li>
</ul>
<p>```bash
  # On macOS:</p>
<blockquote>
<p>open htmlcov/index.html
  ```</p>
</blockquote>
<p><img alt="alt_text" src="unit_tests_figs/image_1.png" /></p>
<ul>
<li>By clicking on a file you can see which lines are not covered</li>
</ul>
<p><img alt="alt_text" src="unit_tests_figs/image_2.png" /></p>
<h3 id="an-example-coverage-session">An example coverage session</h3>
<ul>
<li>
<p>We want to measure the unit test coverage of <code>oms</code> component from both fast
  and slow tests</p>
</li>
<li>
<p>We start by running the fast tests:</p>
</li>
</ul>
<p>```bash
  # Run fast unit tests</p>
<blockquote>
<p>i run_fast_tests --coverage -p oms
  collected 66 items / 7 deselected / 59 selected
  ...</p>
</blockquote>
<p># Compute the coverage for the module sorting by coverage
  docker&gt; coverage report --include="oms/<em>" --omit="</em>/test_*.py" --sort=Cover</p>
<p>Name                                    Stmts   Miss Branch BrPart  Cover</p>
<hr />
<p>oms/locates.py                              7      7      2      0     0%
  oms/oms_utils.py                           34     34      6      0     0%
  oms/tasks.py                                3      3      0      0     0%
  oms/oms_lib_tasks.py                       64     39      2      0    38%
  oms/order.py                              101     30     22      0    64%
  oms/test/oms_db_helper.py                  29     11      2      0    65%
  oms/api.py                                154     47     36      2    70%
  oms/broker.py                             200     31     50      9    81%
  oms/pnl_simulator.py                      326     42     68      8    83%
  oms/place_orders.py                       121      8     18      6    90%
  oms/portfolio.py                          309     21     22      0    92%
  oms/oms_db.py                              47      0     10      3    95%
  oms/broker_example.py                      23      0      4      1    96%
  oms/mr_market.py                           55      1     10      1    97%
  oms/<strong>init</strong>.py                             0      0      0      0   100%
  oms/call_optimizer.py                      31      0      0      0   100%
  oms/devops/<strong>init</strong>.py                      0      0      0      0   100%
  oms/devops/docker_scripts/<strong>init</strong>.py       0      0      0      0   100%
  oms/order_example.py                       26      0      0      0   100%
  oms/portfolio_example.py                   32      0      0      0   100%</p>
<hr />
<p>TOTAL                                    1562    274    252     30    80%
  ```</p>
<ul>
<li>
<p>We see that certain files have a low coverage, so we want to see what is not
  covered.</p>
</li>
<li>
<p>Generate the same report in a browsable format</p>
</li>
</ul>
<p><code>``bash
  docker&gt; rm -rf htmlcov; coverage html --include="oms/*" --omit="*/test_*.py"
  # Wrote HTML report to</code>htmlcov/index.html`</p>
<blockquote>
<p>open htmlcov/index.html
  ```</p>
</blockquote>
<ul>
<li>
<p>The low coverage for <code>tasks.py</code> and <code>oms_lib_tasks.py</code> is due to the fact that
  we are running code through invoke that doesn't allow <code>coverage</code> to track it.</p>
</li>
<li>
<p>Now we run the coverage for the slow tests</p>
</li>
</ul>
<p>```bash
  # Save the coverage from the fast tests run</p>
<blockquote>
<p>cp .coverage .coverage_fast_tests</p>
<p>i run_slow_tests --coverage -p oms
  collected 66 items / 59 deselected / 7 selected</p>
<p>cp .coverage .coverage_slow_tests</p>
<p>coverage report --include="oms/<em>" --omit="</em>/test_*.py" --sort=Cover</p>
</blockquote>
<p>Name                                    Stmts   Miss Branch BrPart  Cover</p>
<hr />
<p>oms/locates.py                              7      7      2      0     0%</p>
<p>oms/oms_utils.py                           34     34      6      0     0%</p>
<p>oms/tasks.py                                3      3      0      0     0%</p>
<p>oms/pnl_simulator.py                      326    280     68      1    13%</p>
<p>oms/place_orders.py                       121    100     18      0    15%</p>
<p>oms/mr_market.py                           55     44     10      0    17%</p>
<p>oms/portfolio.py                          309    256     22      0    18%</p>
<p>oms/call_optimizer.py                      31     25      0      0    19%</p>
<p>oms/broker.py                             200    159     50      0    20%</p>
<p>oms/order.py                              101     78     22      0    20%</p>
<p>oms/order_example.py                       26     19      0      0    27%</p>
<p>oms/broker_example.py                      23     14      4      0    33%</p>
<p>oms/portfolio_example.py                   32     21      0      0    34%</p>
<p>oms/api.py                                154    107     36      0    36%</p>
<p>oms/oms_lib_tasks.py                       64     39      2      0    38%</p>
<p>oms/oms_db.py                              47      5     10      2    84%</p>
<p>oms/<strong>init</strong>.py                             0      0      0      0   100%</p>
<p>oms/devops/<strong>init</strong>.py                      0      0      0      0   100%</p>
<p>oms/devops/docker_scripts/<strong>init</strong>.py       0      0      0      0   100%</p>
<p>oms/test/oms_db_helper.py                  29      0      2      0   100%</p>
<hr />
<p>TOTAL                                    1562   1191    252      3    23%
  ```</p>
<ul>
<li>We see that the coverage from the slow tests is only 23% for 7 tests</li>
</ul>
<p><code>bash
  root@6faaa979072e:/app/amp# coverage combine .coverage_fast_tests .coverage_slow_tests
  Combined data file .coverage_fast_tests
  Combined data file .coverage_slow_tests</code></p>
<h3 id="an-example-with-customized-pytest-cov-html-run">An example with customized <code>pytest-cov</code> html run</h3>
<ul>
<li>
<p>We want to measure unit test coverage specifically for one test in
  <code>im_v2/common/data/transform/</code> and to save generated htmlcov in the same
  directory.</p>
</li>
<li>
<p>Run command below after <code>i docker_bash</code>:</p>
</li>
</ul>
<p><code>bash
  pytest --cov-report term-missing
  --cov=im_v2/common/data/transform/ im_v2/common/data/transform/test/test_transform_utils.py
  --cov-report html:im_v2/common/data/transform/htmlcov \</code></p>
<ul>
<li>Output sample:</li>
</ul>
<p><code>bash
  ---------- coverage: platform linux, python 3.8.10-final-0 ----------- Name Stmts Miss Cover Missing ----------------------------------------------------------------------------------------------- im_v2/common/data/transform/convert_csv_to_pq.py 55 55 0% 2-159 im_v2/common/data/transform/extract_data_from_db.py 55 55 0% 2-125 im_v2/common/data/transform/pq_convert.py 126 126 0% 3-248 im_v2/common/data/transform/transform_pq_by_date_to_by_asset.py 131 131 0% 2-437 im_v2/common/data/transform/transform_utils.py 22 0 100% ----------------------------------------------------------------------------------------------- TOTAL 389 367 6% Coverage HTML written to dir im_v2/common/data/transform/htmlcov \</code></p>
<h3 id="generate-coverage-report-with-invoke">Generate coverage report with <code>invoke</code></h3>
<ul>
<li>One can compute test coverage for a specified directory and generate text and
  HTML reports automatically using <code>invoke task run_coverage_report</code></li>
</ul>
<p>```bash</p>
<blockquote>
<p>i --help run_coverage_report
  INFO: &gt; cmd='/data/grisha/src/venv/amp.client_venv/bin/invoke --help run_coverage_report'</p>
</blockquote>
<p>Usage: inv[oke] [--core-opts] run_coverage_report [--options] [other tasks here ...]</p>
<p>Docstring:</p>
<pre><code>Compute test coverage stats.

The flow is:
</code></pre>
<ul>
<li>
<p>Run tests and compute coverage stats for each test type</p>
</li>
<li>
<p>Combine coverage stats in a single file</p>
</li>
<li>
<p>Generate a text report</p>
</li>
<li>
<p>Generate a HTML report (optional)</p>
</li>
<li>
<p>Post it on S3 (optional)</p>
</li>
</ul>
<p>:param target_dir: directory to compute coverage stats for <a href="#running-unit-tests">here</a></p>
<p>:param generate_html_report: whether to generate HTML coverage report or not</p>
<p>:param publish_html_on_s3: whether to publish HTML coverage report or not</p>
<p>:param aws_profile: the AWS profile to use for publishing HTML report</p>
<p>Options:
    -a STRING, --aws-profile=STRING</p>
<pre><code>-g, --[no-]generate-html-report

-p, --[no-]publish-html-on-s3

-t STRING, --target-dir=STRING
</code></pre>
<p>```</p>
<h4 id="common-usage">Common usage</h4>
<ul>
<li>Compute coverage for <code>market_data</code> dir, generate text and HTML reports and
  publish HTML report on S3</li>
</ul>
<p>```bash</p>
<blockquote>
<p>i run_coverage_report --target-dir market_data
  ...
  Name                                   Stmts   Miss Branch BrPart  Cover</p>
</blockquote>
<hr />
<p>market_data/real_time_market_data.py     100     81     32      0    16%</p>
<p>market_data/replayed_market_data.py      111     88     24      0    19%</p>
<p>market_data/abstract_market_data.py      177    141     24      0    19%</p>
<p>market_data/market_data_example.py       124     97     10      0    20%</p>
<p>market_data/market_data_im_client.py      66     50     18      0    21%</p>
<p>market_data/<strong>init</strong>.py                    5      0      0      0   100%</p>
<hr />
<p>TOTAL                                    583    457    108      0    19%
  Wrote HTML report to htmlcov/index.html</p>
<p>20:08:53 - INFO  lib_tasks.py _publish_html_coverage_report_on_s3:3679  HTML coverage report is published on S3: path=<code>s3://cryptokaizen-html/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project</code>
  ```</p>
<h3 id="publishing-html-report-on-s3">Publishing HTML report on S3</h3>
<ul>
<li>To make a dir with the report unique linux user and Git branch name are used,
  e.g.,
  <code>html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project</code></li>
<li><code>html_coverage</code> is the common dir on S3 for coverage reports</li>
<li>After publishing the report, one can easily open it via a local web browser</li>
<li>See the details in
    <a href="https://docs.google.com/document/d/1JWAbNLuwseLAleW1wQ0BH8gxHEA9Hb9ZRgAy-Ooj9-M/edit#heading=h.58pd2t5yyzlw">htmlcov server</a></li>
<li>E.g.
    <a href="http://172.30.2.44/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project/">http://172.30.2.44/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project/</a></li>
</ul>
<h1 id="running-pytest-directly">Running <code>pytest</code> directly</h1>
<h2 id="usage-and-invocations-reference">Usage and Invocations reference</h2>
<ul>
<li>See <a href="http://doc.pytest.org/en/latest/usage.html"><code>pytest</code> documentation</a></li>
<li>Some examples of useful command lines:</li>
</ul>
<p>```bash
  # Stop at first failure</p>
<blockquote>
<p>pytest -x</p>
</blockquote>
<p># Run a single class</p>
<blockquote>
<p>pytest -k TestPcaFactorComputer1</p>
</blockquote>
<p># Run a single test method</p>
<blockquote>
<p>pytest core/test/test_core.py::TestPcaFactorComputer1::test_linearize_eigval_eigvec</p>
</blockquote>
<p># Remove cache artifacts</p>
<blockquote>
<p>find . -name "<strong>pycache</strong>" -o -name ".pytest_cache"</p>
</blockquote>
<p>./.pytest_cache</p>
<p>./dev_scripts/test/Test_linter_py1.test_linter1/tmp.scratch/<strong>pycache</strong></p>
<p>./dev_scripts/test/<strong>pycache</strong></p>
<p>./dev_scripts/<strong>pycache</strong></p>
<blockquote>
<p>find . -name "<strong>pycache</strong>" -o -name ".pytest_cache" | xargs rm -rf</p>
</blockquote>
<p># Run with a clear cache</p>
<blockquote>
<p>pytest --cache-clear</p>
</blockquote>
<p># Run the tests that last failed (this data is stored in .pytest_cache/v/cache/lastfailed)</p>
<blockquote>
<p>pytest --last-failed
  ```</p>
</blockquote>
<h2 id="custom-pytest-options-behaviors">Custom <code>pytest</code> options behaviors</h2>
<h3 id="enable-logging">Enable logging</h3>
<ul>
<li>To enable logging of <code>_LOG.debug</code> for a single test run:</li>
</ul>
<p>```bash
  # Enable debug info</p>
<blockquote>
<p>pytest oms/test/test_broker.py::TestSimulatedBroker1 -s --dbg
  ```</p>
</blockquote>
<h3 id="update-golden-outcomes">Update golden outcomes</h3>
<ul>
<li>This switch allows to overwrite the golden outcomes that are used as reference
  in the unit tests to detect failures</li>
</ul>
<p>```bash</p>
<blockquote>
<p>pytest --update_outcomes
  ```</p>
</blockquote>
<h3 id="incremental-test-mode">Incremental test mode</h3>
<ul>
<li>
<p>This switch allows to reuse artifacts in the test directory and to skip the
  clean up phase</p>
</li>
<li>
<p>It is used to re-run tests from the middle when they are very long and one
  wants to debug them</p>
</li>
</ul>
<p>```bash</p>
<blockquote>
<p>pytest --incremental
  ```</p>
</blockquote>
<h2 id="debugging-notebooks">Debugging Notebooks</h2>
<ol>
<li>run a failing test with <code>-s --dbg</code> to get detailed logs</li>
<li>e.g., <code>&gt; pytest core/plotting/test/test_gallery_notebook.py -s --dbg</code></li>
<li>from the logs take a <code>run_notebook.py</code> script command that was run by the
   test</li>
<li>e.g., starting like
     <code>/app/dev_scripts/notebooks/run_notebook.py --notebook ...</code></li>
<li>Append <code>--no_suppress_output</code> to this command and run it again directly from
   the bash</li>
<li>e.g., like
     <code>&gt; /app/dev_scripts/notebooks/run_notebook.py --notebook ... --no_suppress_output</code></li>
<li>scroll up the logs and see a report about the problem, notebooks failures
   will be displayed as well</li>
<li>e.g.,<br />
      <img width="756" src="https://github.com/sorrentum/sorrentum/assets/31514660/43a2854e-ae4e-450d-95fd-f16df0a53c79"></li>
</ol>
<h1 id="running-tests-on-gh-actions">Running tests on GH Actions</h1>
<ul>
<li>The official documentation is
  <a href="https://docs.github.com/en/actions">https://docs.github.com/en/actions</a></li>
</ul>
<h2 id="how-to-run-a-single-test-on-gh-action">How to run a single test on GH Action</h2>
<ul>
<li>
<p>Unfortunately there is no way to log in and run interactively on GH machines.
  This is a feature requested but not implemented by GH yet.</p>
</li>
<li>
<p>All the code to run GH Actions is in the <code>.github</code> directory in <code>lemonade</code> and
  <code>amp</code>.</p>
</li>
<li>
<p>E.g., to run a single test in the fast test target, instead of the entire
  regression suite</p>
</li>
<li>
<p>You can modify <code>.github/workflows/fast_tests.yml</code>, by replacing</p>
</li>
</ul>
<p><code>bash
   # run: invoke run_fast_tests
   run: invoke run_fast_tests --pytest-opts="helpers/test/test_git.py::Test_git_modified_files1::test_get_modified_files_in_branch1 -s --dbg"</code></p>
<ul>
<li>
<p>Note that the indentation matters, since it's a YAML file</p>
<p><img alt="alt_text" src="unit_tests_figs/image_3.png" /></p>
</li>
<li>
<p>The <code>-s --dbg</code> is to show <code>_LOG.debug</code> in case you care about that to get
     more information</p>
</li>
<li>
<p>Commit the code to your branch (not in master please) since GH runs each
   branch independently</p>
</li>
<li>Kick off manually the fast test through the GH interface</li>
<li>After debugging, you can revert the change from your branch to <code>master</code> and
   move along with the usual PR flow</li>
</ul>
<h1 id="guidelines-about-writing-unit-tests">Guidelines about writing unit tests</h1>
<h3 id="what-is-a-unit-test"><strong>What is a unit test?</strong></h3>
<ul>
<li>A unit test is a small, self-contained test of a public function or public
  method of a library</li>
<li>The test specifies the given inputs, any necessary state, and the expected
  output</li>
<li>Running the test ensures that the actual output agrees with the expected
  output</li>
</ul>
<h3 id="why-is-unit-testing-important"><strong>Why is unit testing important?</strong></h3>
<ul>
<li>Unit testing is an integral part of Pragmatic programming approach</li>
<li>Some of the tips that relate to it are:</li>
<li>Design with Contracts</li>
<li>Refactor Early, Refactor Often</li>
<li>Test Your Software, or Your Users Will</li>
<li>Coding Ain't Done Till All the Tests Run</li>
<li>Test State Coverage, Not Code Coverage</li>
<li>You Can't Write Perfect Software</li>
<li>Crash Early</li>
<li>Design to Test</li>
<li>Test Early. Test Often. Test Automatically.</li>
<li>Use Saboteurs to Test Your Testing</li>
<li>Find Bugs Once</li>
<li>Good unit testing improves software quality. It does this in part by</li>
<li>Eliminating bugs (obvious)</li>
<li>Clarifying code design and interfaces ("Design to Test")</li>
<li>Making refactoring safer and easier ("Refactor Early, Refactor Often")</li>
<li>Documenting expected behavior and usage</li>
</ul>
<h2 id="unit-testing-tips">Unit testing tips</h2>
<h3 id="tip-1-test-one-thing">Tip #1: Test one thing</h3>
<ul>
<li>A good unit test tests only one thing</li>
<li>Testing one thing keeps the unit test simple, relatively easy to understand,
  and helps isolate the root cause when the test fails</li>
<li>How do we test more than one thing? By having more than one unit test!</li>
</ul>
<h3 id="tip-2-keep-tests-self-contained">Tip #2: Keep tests self-contained</h3>
<ul>
<li>A unit test should be independent of all other unit tests</li>
<li>Each test should be self-sufficient</li>
<li>Additionally, one should never assume that unit tests will be executed in a
  particular order</li>
<li>A corollary of keeping tests self-contained is to keep all information needed
  to understand the test within the test itself</li>
<li>In other words, when possible, avoid calling helper functions to load data or
  state to initialize the test; instead, specify the data explicitly in the test
  where it is used</li>
<li>This makes the test easier to understand and easier to debug when it fails</li>
<li>If multiple unit tests use or can use the same initialization data, do not
    hesitate repeating it in each test (or consider using parameterized testing)</li>
</ul>
<h3 id="tip-3-only-specify-data-related-to-what-is-being-tested">Tip #3: Only specify data related to what is being tested</h3>
<ul>
<li>If a function that is being tested supports optional arguments, but those
  optional arguments are not needed for a particular unit test, then do not
  specify them in the test</li>
<li>Specify the minimum of what is required to test what is being tested.</li>
</ul>
<h3 id="tip-4-test-realistic-corner-cases">Tip #4: Test realistic corner cases</h3>
<ul>
<li>Can your function receive a list that is empty?</li>
<li>Can it return an empty Series?</li>
<li>What happens if it receives a numerical value outside of an expected range?</li>
<li>How should the function behave in those cases? Should it crash? Should it
  return a reasonable default value?</li>
<li>Expect these questions to come up in practice and think through what the
  appropriate behavior should be. Then, test for it.</li>
</ul>
<h3 id="tip-5-test-a-typical-scenario">Tip #5: Test a typical scenario</h3>
<ul>
<li>In ensuring that corner cases are covered, do not overlook testing basic
  functionality for typical cases</li>
<li>This is useful for verifying current behavior and to support refactoring.</li>
</ul>
<h3 id="tip-6-test-executable-scripts-end-to-end">Tip #6: Test executable scripts end-to-end</h3>
<ul>
<li>In some cases, like scripts, it is easy to get lost chasing the coverage %</li>
<li>E.g. covering every line of the original, including the parser</li>
<li>This is not always necessary</li>
<li>If you are able to run a script with all arguments present, it means that
    the parser works correctly</li>
<li>So an end-to-end smoke test will also cover the parser</li>
<li>This saves a little time and reduces the bloat</li>
<li>If you need to test the functionality, consider factoring out as much code as
  possible from <code>_main</code></li>
<li>A good practice is to have a <code>_run</code> function that does all the job and
    <code>_main</code> only brings together the parser and the executable part</li>
</ul>
<h2 id="conventions">Conventions</h2>
<h3 id="naming-and-placement-conventions">Naming and placement conventions</h3>
<ul>
<li>We follow the convention (that happen to be mostly the default to <code>pytest</code>):</li>
<li>A directory <code>test</code> that contains all the test code and artifacts</li>
<li>The <code>test</code> directory contains all the <code>test_*.py</code> files and all inputs and
    outputs for the tests.</li>
<li>A unit test file should be close to the library / code it tests</li>
<li>The test class should make clear reference to the code that is tested</li>
<li>To test a class <code>FooBar</code> the corresponding test class is named
    <code>TestFooBar,</code>we use the CamelCase for the test classes</li>
<li>You can add a number, e.g., <code>TestFooBar1()</code>, if there are multiple test
    classes that are testing the code in different ways (e.g.,<code>setUp()</code>
    <code>tearDown()</code> are different)</li>
<li>To test a function <code>generate_html_tables()</code> the corresponding test class is
    <code>Test_generate_html_tables</code></li>
<li>It's ok to have multiple test methods, e.g., for <code>FooBar.method_a()</code> and
  <code>FooBar.method_b()</code>, the test method is:
  <code>python
  class TestFooBar1(unittest2.TestCase):
      def test_method_a(self):
          ...
      def test_method_b(self):
          ...</code></li>
<li>Split test classes and methods in a reasonable way, so each one tests one
  single thing in the simplest possible way</li>
<li>Remember that test code is not second class citizen, although it's auxiliary
  to the code</li>
<li>Add comments and docstring explaining what the code is doing</li>
<li>If you change the name of a class, also the test should be changed</li>
<li>If you change the name of a file also the name of the file with the testing
    code should be changed</li>
</ul>
<h3 id="our-framework-to-test-using-input-output-data">Our framework to test using input / output data</h3>
<ul>
<li><code>helpers/unit_test.py</code> has some utilities to easily create input and output
  dirs storing data for unit tests</li>
<li><code>hut.TestCase</code> has various methods to help you create</li>
<li><code>get_input_dir</code>: return the name of the dir used to store the inputs</li>
<li><code>get_scratch_space</code>: return the name of a scratch dir to keep artifacts of
    the test</li>
<li><code>get_output_dir</code>: probably not interesting for the user</li>
<li>The directory structure enforced by the out <code>TestCase</code> is like:</li>
</ul>
<p>```bash</p>
<blockquote>
<p>tree -d edg/form_8/test/
  edg/form_8/test/
  â””â”€â”€ TestExtractTables1.test1
      â”œâ”€â”€ input
      â””â”€â”€ output
  ```</p>
</blockquote>
<ul>
<li>The layout of test dir:
  ```bash<blockquote>
<p>ls -1 helpers/test/
  Test_dassert1.test2
  Test_dassert1.test3
  Test_dassert1.test4
  ...
  Test_dassert_misc1.test6
  Test_dassert_misc1.test8
  Test_system1.test7
  test_dbg.py
  test_helpers.py
  test_system_interaction.py
  ```</p>
</blockquote>
</li>
</ul>
<h3 id="use-text-and-not-pickle-files-as-input">Use text and not pickle files as input</h3>
<ul>
<li>The problem with pickle files are the usual ones</li>
<li>They are not stable across different version of libraries</li>
<li>They are not human readable</li>
<li>Prefer to use text file</li>
<li>E.g., use a CSV file</li>
<li>If the data used for testing is generated in a non-complicated way</li>
<li>Document how it was generated</li>
<li>Even better add a test that generates the data</li>
<li>Use a subset of the input data</li>
<li>The smaller the better for everybody<ul>
<li>Fast tests</li>
<li>Easier to debug</li>
<li>More targeted unit test</li>
</ul>
</li>
<li>Do not check in 1 megabyte of test data!</li>
</ul>
<h3 id="check_string-vs-selfassertequal"><code>check_string</code> vs <code>self.assertEqual</code></h3>
<ul>
<li>TODO(gp): Add</li>
</ul>
<h3 id="use-selfassert_equal">Use <code>self.assert_equal</code></h3>
<ul>
<li>This is a function that helps you understand what are the mismatches</li>
<li>It works on <code>str</code></li>
</ul>
<h3 id="how-to-split-unit-test-code-in-files">How to split unit test code in files</h3>
<ul>
<li>The two extreme approaches are:</li>
<li>All the test code for a directory goes in one file
    <code>foo/bar/test/test_$DIRNAME.py</code> (or <code>foo/bar/test/test_all.py</code>)</li>
<li>Each file <code>foo/bar/$FILENAME</code> with code gets its corresponding
    <code>foo/bar/test/test_$FILENAME.py</code><ul>
<li>It should also be named according to the library it tests</li>
<li>For example, if the library to test is called <code>pnl.py</code>, then the
  corresponding unit test should be called <code>test_pnl.py</code></li>
</ul>
</li>
<li>Pros of 1) vs 2)</li>
<li>Less maintenance churn<ul>
<li>It takes work to keep the code and the test files in sync, e.g.,</li>
<li>If you change the name of the code file you don't have to change other
    file names</li>
<li>If you move one class from one file to another, you might not need to
    move test code</li>
</ul>
</li>
<li>Fewer files opened in your editor</li>
<li>Avoid many files with a lot of boilerplate code</li>
<li>Cons of 1) vs 2)</li>
<li>The single file can become huge!</li>
<li>Compromise solution: Start with a single file
  <code>test_$DIRNAME.py</code>(or<code>test*dir_name.py</code>) * In the large file add a framed
  comment like:
  <code>python
  # ##################
  # Unit tests for â€¦
  # ##################</code></li>
<li>So it's easy to find which file is tested were using grep</li>
<li>Then split when it becomes too big using <code>test_$FILENAME.py</code></li>
</ul>
<h3 id="skeleton-for-unit-test">Skeleton for unit test</h3>
<ul>
<li>Interesting unit tests are in <code>helpers/test</code></li>
<li>A unit test looks like:
  <code>python
  import helpers.unit_test as hut
  class Test...(hut.TestCase):
      def test...(self):
          ...</code></li>
<li><code>pytest</code> will take care of running the code so you don't need:
  <code>python
  if __name__ == '__main__':
  unittest.main()</code></li>
</ul>
<h3 id="hierarchical-testcase-approach">Hierarchical <code>TestCase</code> approach</h3>
<ul>
<li>Whenever there is hierarchy in classes, we also create a hierarchy of test
  classes</li>
<li>A parent test class looks like:
  <code>python
  import helpers.unit_test as hut
  class SomeClientTestCase(hut.TestCase):
      def _test...1(self):
          ...
      def _test...2(self):
          ...</code></li>
<li>While a child test class looks like this where test methods use the
  corresponding methods from the parent test class:
  <code>python
  class TestSomeClient(SomeClientTestCase):
      def test...1(self):
          ...
      def test...2(self):
          ...</code></li>
<li>Each <code>TestCase</code> tests a "behavior" like a set of related methods</li>
<li>Each <code>TestCase</code> is under the test dir</li>
<li>Each derived class should use the proper <code>TestCase</code> classes to reach a decent
  coverage</li>
<li>It is OK to use non-private methods in test classes to ensure that the code is
  in order of dependency, so that the reader doesn't have to jump back / forth</li>
<li>We want to separate chunks of unit test code using:</li>
</ul>
<p><code>python
  # ########################################################################</code></p>
<p>putting all the methods used by that chunk at the beginning and so on</p>
<ul>
<li>It is OK to skip a <code>TestCase</code> method if not meaningful, when coverage is
  enough</li>
<li>As an example, see <code>im_v2/common/data/client/test/im_client_test_case.py</code> and
  <code>im_v2/ccxt/data/client/test/test_ccxt_clients.py</code></li>
</ul>
<h3 id="use-the-appropriate-selfassert">Use the appropriate <code>self.assert*</code></h3>
<ul>
<li>When you get a failure you don't want to get something like "True is not
  False", rather an informative message like "5 is not &lt; 4"</li>
<li>Bad \
  <code>self.assertTrue(a &amp;lt; b)</code></li>
<li>Good \
  <code>self.assertLess(a, b)</code></li>
</ul>
<h3 id="do-not-use-hdbgdassert">Do not use <code>hdbg.dassert</code></h3>
<ul>
<li><code>dassert</code> are for checking self-consistency of the code</li>
<li>The invariant is that you can remove <code>dbg.dassert</code> without changing the
  behavior of the code. Of course you can't remove the assertion and get unit
  tests to work</li>
</ul>
<h3 id="interesting-testing-functions">Interesting testing functions</h3>
<ul>
<li>
<p>List of useful testing functions are:</p>
</li>
<li>
<p><a href="https://docs.python.org/2/library/unittest.html#test-cases">General python</a></p>
</li>
<li><a href="https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.testing.html">Numpy</a></li>
<li><a href="https://pandas.pydata.org/pandas-docs/version/0.21/api.html#testing-functions">Pandas</a></li>
</ul>
<h3 id="use-setupteardown">Use setUp/tearDown</h3>
<ul>
<li>If you have a lot of repeated code in your tests, you can make them shorter by
  moving this code to <code>setUp/tearDown</code>methods:</li>
<li><code>setUp()</code> \
    Method called to prepare the test fixture. This is called immediately before
    calling the test method; other than <a href="https://docs.python.org/3/library/exceptions.html#AssertionError"><code>AssertionError</code></a>
    or <a href="https://docs.python.org/3/library/unittest.html#unittest.SkipTest"><code>SkipTest</code></a>,
    any exception raised by this method will be considered an error rather than a
    test failure. The default implementation does nothing.</li>
<li><code>tearDown()</code> \
    Method called immediately after the test method has been called and the result
    recorded. This is called even if the test method raised an exception, so the
    implementation in subclasses may need to be particularly careful about checking
    internal state. Any exception, other than <a href="https://docs.python.org/3/library/exceptions.html#AssertionError"><code>AssertionError</code></a>
    or <a href="https://docs.python.org/3/library/unittest.html#unittest.SkipTest"><code>SkipTest</code></a>,
    raised by this method will be considered an additional error rather than a test
    failure (thus increasing the total number of reported errors). This method will
    only be called if the <a href="https://docs.python.org/3/library/unittest.html#unittest.TestCase.setUp"><code>setUp()</code></a>
    succeeds, regardless of the outcome of the test method. The default implementation
    does nothing.</li>
<li>
<p>If you need some expensive code parts to be done once for the whole test
  class, such as opening a database connection, opening a temporary file on the
  filesystem, loading a shared library for testing, etc., you can use
  <code>setUpClass/tearDownClass</code> methods:</p>
</li>
<li>
<p><code>setUpClass()</code></p>
<p>A class method called before tests in an individual class are run.
<code>setUpClass</code> is called with the class as the only argument and must be
decorated as a
<a href="https://docs.python.org/3/library/functions.html#classmethod">classmethod()</a>:</p>
<p><code>python
@classmethod
def setUpClass(cls):
    ...</code></p>
</li>
<li>
<p><code>tearDownClass()</code></p>
<p>A class method called after tests in an individual class have run.
<code>tearDownClass</code> is called with the class as the only argument and must be
decorated as a
<a href="https://docs.python.org/3/library/functions.html#classmethod">classmethod()</a>:</p>
<p><code>python
@classmethod
def tearDownClass(cls):
    ...</code></p>
</li>
<li>
<p>For more information see
  <a href="https://docs.python.org/3/library/unittest.html">official unittest docs</a></p>
</li>
</ul>
<h1 id="update-test-tags">Update test tags</h1>
<ul>
<li>There are 2 files with the list of tests' tags:</li>
<li><code>amp/pytest.ini</code></li>
<li><code>.../pytest.ini (if</code>amp<code>is a submodule)</code></li>
<li>In order to update the tags (do it in the both files):</li>
<li>In the <code>markers</code> section add a name of a new tag</li>
<li>After a <code>:</code> add a short description</li>
<li>Keep tags in the alphabetical order</li>
</ul>
<h1 id="mocking">Mocking</h1>
<h2 id="refs">Refs</h2>
<ul>
<li>Introductory article is
  <a href="https://realpython.com/python-mock-library/">https://realpython.com/python-mock-library/ </a></li>
<li>Official Python documentation for the mock package can be seen here
  <a href="https://docs.python.org/3/library/unittest.mock.html">unit test mock</a></li>
</ul>
<h2 id="common-usage-samples">Common usage samples</h2>
<p>Best to apply on any part that is deemed unnecessary for specific test</p>
<ul>
<li>Complex functions</li>
<li>Mocked functions can be tested separately</li>
<li>3rd party provider calls</li>
<li>CCXT</li>
<li>Talos</li>
<li>AWS<ul>
<li>S3</li>
<li>See <code>helpers/hmoto.py</code> in <code>cmamp</code> repo</li>
<li>Secrets</li>
<li>Etc...</li>
</ul>
</li>
<li>
<p>DB calls</p>
</li>
<li>
<p>There are many more possible combinations that can be seen in official
  documentation.</p>
</li>
<li>Below are the most common ones for basic understanding.</li>
</ul>
<h2 id="philosophy-about-mocking">Philosophy about mocking</h2>
<ol>
<li>We want to mock the minimal surface of a class</li>
<li>E.g., assume there is a class that is interfacing with an external provider
     and our code places requests and get values back</li>
<li>We want to replace the provider with an object that responds to the
     requests with the actual response of the provider</li>
<li>In this way we can leave all the code of our class untouched and tested</li>
<li>We want to test public methods of our class (and a few private methods)</li>
<li>In other words, we want to test the end-to-end behavior and not how things
     are achieved</li>
<li>Rationale: if we start testing "how" things are done and not "what" is
     done, we can't change how we do things (even if it doesn't affect the
     interface and its behavior), without updating tons of methods</li>
<li>We want to test the minimal amount of behavior that enforces what we care
     about</li>
</ol>
<h2 id="some-general-suggestions-about-testing">Some general suggestions about testing</h2>
<h3 id="test-from-the-outside-in">Test from the outside-in</h3>
<ul>
<li>We want to start testing from the end-to-end methods towards the constructor
  of an object</li>
<li>Rationale: often we start testing very carefully the constructor and then we
  get tired / run out of time when we finally get to test the actual behavior</li>
<li>Also testing the important behavior automatically tests building the objects</li>
<li>Use the code coverage to see what's left to test once you have tested the
  "most external" code</li>
</ul>
<h3 id="we-dont-need-to-test-all-the-assertions">We don't need to test all the assertions</h3>
<ul>
<li>E.g., testing carefully that we can't pass a value to a constructor doesn't
  really test much besides the fact that <code>dassert</code> works (which surprisingly
  works!)</li>
<li>We don't care about line coverage or checking boxes for the sake of checking
  boxes</li>
</ul>
<h3 id="use-strings-to-compare-output-instead-of-data-structures">Use strings to compare output instead of data structures</h3>
<ul>
<li>Often it's easier to do a check like:</li>
</ul>
<p>```python
  # Better:
  expected = str(...)
  expected = pprint.pformat(...)</p>
<p># Worse:
  expected = ["a", "b", { ... }]
  ```</p>
<p>rather than building the data structure</p>
<ul>
<li>Some purists might not like this, but</li>
<li>It's much faster to use a string (which is or should be one-to-one to the
    data structure), rather than the data structure itself<ul>
<li>By extension, many of the more complex data structure have a built-in
  string representation</li>
</ul>
</li>
<li>It is often more readable, easier to diff (e.g., self.assertEqual vs
    self.assert_equal)</li>
<li>In case of mismatch it's easier to update the string with copy-paste rather
    than creating a data structure that matches what was created</li>
</ul>
<h3 id="use-selfcheck_string-for-things-that-we-care-about-not-changing-or-are-too-big-to-have-as-strings-in-the-code">Use <code>self.check_string()</code> for things that we care about not changing (or are too big to have as strings in the code)</h3>
<ul>
<li>Use <code>self.assert_equal()</code> for things that should not change (e.g., 1 + 1 = 2)</li>
<li>When using check_string still try to add invariants that force the code to be
  correct</li>
<li>E.g., if we want to check the PnL of a model we can freeze the output with
  <code>check_string()</code> but we want to add a constraint like there are more
  timestamps than 0 to avoid the situation where we update the string to
  something malformed</li>
</ul>
<h3 id="each-test-method-should-test-a-single-test-case">Each test method should test a single test case</h3>
<ul>
<li>Rationale: we want each test to be clear, simple, fast</li>
<li>If there is repeated code we should factor it out (e.g., builders for objects)</li>
</ul>
<h3 id="each-test-should-be-crystal-clear-on-how-it-is-different-from-the-others">Each test should be crystal clear on how it is different from the others</h3>
<ul>
<li>Often you can factor out all the common logic into an helper method</li>
<li>Copy-paste is not allowed in unit tests in the same way it's not allowed in
  production code</li>
</ul>
<h3 id="in-general-you-want-to-budget-the-time-to-write-unit-tests">In general you want to budget the time to write unit tests</h3>
<ul>
<li>E.g., "I'm going to spend 3 hours writing unit tests". This is going to help
  you focus on what's important to test and force you to use an iterative
  approach rather than incremental (remember the Monalisa)</li>
</ul>
<p><img alt="alt_image" src="unit_tests_figs/image_4.png" /></p>
<h3 id="write-skeleton-of-unit-tests-and-ask-for-a-review-if-you-are-not-sure-how-what-to-test">Write skeleton of unit tests and ask for a review if you are not sure how what to test</h3>
<ul>
<li>Aka "testing plan"</li>
</ul>
<h2 id="object-patch-with-return-value">Object patch with return value</h2>
<pre><code class="language-python">import unittest.mock as umock
import im_v2.ccxt.data.extract.extractor as ivcdexex

@umock.patch.object(ivcdexex.hsecret, &quot;get_secret&quot;)
def test_function_call1(self, mock_get_secret: umock.MagicMock):
    mock_get_secret.return_value = &quot;dummy&quot;
</code></pre>
<ul>
<li>Function <code>get_secret</code> in <code>helpers/hsecret.py</code> is mocked</li>
<li>Pay attention on where is <code>get_secret</code> mocked:<ul>
<li>It is mocked in im_v2.ccxt.data.extract.extractor as â€œget_secretâ€ is
  called there in function that is being tested</li>
</ul>
</li>
<li><code>@umock.patch.object(hsecret, "get_secret")</code> will not work as mocks are
    applied after all modules are loaded, hence the reason for using exact
    location<ul>
<li>If we import module in test itself it will work as mock is applied</li>
<li>For modules outside of test function it is too late as they are loaded
  before mocks for test are applied</li>
</ul>
</li>
<li>On every call it returns string "dummy"</li>
</ul>
<h2 id="path-patch-with-multiple-return-values">Path patch with multiple return values</h2>
<pre><code class="language-python">import unittest.mock as umock

@umock.patch(&quot;helpers.hsecret.get_secret&quot;)
def test_function_call1(self, mock_get_secret: umock.MagicMock):

mock_get_secret.side_effect = [&quot;dummy&quot;, Exception]
</code></pre>
<ul>
<li>On first call, string <code>dummy</code> is returned</li>
<li>On second, <code>Exception</code> is raised</li>
</ul>
<h2 id="ways-of-calling-patch-and-patchobject">Ways of calling <code>patch</code> and <code>patch.object</code></h2>
<ul>
<li>Via decorator
  <code>python
  @umock.patch("helpers.hsecret.get_secret")
  def test_function_call1(self, mock_get_secret: umock.MagicMock):
      pass</code></li>
<li>In actual function
  <code>python
  get_secret_patch = umock.patch("helpers.hsecret.get_secret")
  get_secret_mock = get_secret_patch.start()</code></li>
<li>This is the only approach in which you need to start/stop patch!<ul>
<li>The actual mock is returned as the return value of <code>start()</code> method!</li>
</ul>
</li>
<li>In other two approaches start/stop is handled under the hood and we are
    always interacting with <code>MagicMock</code> object</li>
<li>Via <code>with</code> statement (also in function)
  <code>python
  with umock.patch(""helpers.hsecret.get_secret"") as get_secret_mock:
      pass</code></li>
<li>One of the use cases for this is if we are calling a different function
    inside a function that is being mocked<ul>
<li>Mostly because it is easy for an eye if there are to much patches via
  decorator and we do not need to worry about reverting the patch changes as
  that is automatically done at the end of with statement</li>
</ul>
</li>
</ul>
<h2 id="mock-object-state-after-test-run">Mock object state after test run</h2>
<pre><code class="language-python">@umock.patch.object(exchange_class._exchange, &quot;fetch_ohlcv&quot;)
def test_function_call1(self, fetch_ohlcv_mock: umock.MagicMock):
    self.assertEqual(fetch_ohlcv_mock.call_count, 1)
    actual_args = tuple(fetch_ohlcv_mock.call_args)
    expected_args = (
            (&quot;BTC/USDT&quot;,),
            {&quot;limit&quot;: 2, &quot;since&quot;: 1, &quot;timeframe&quot;: &quot;1m&quot;},
    )
    self.assertEqual(actual_args, expected_args)
</code></pre>
<ul>
<li>After <code>fetch_ohlcv</code> is patched, <code>Mock</code> object is passed to test</li>
<li>In this case, it is <code>fetch_ohlcv_mock</code></li>
<li>
<p>From sample we can see that function is called once</p>
</li>
<li>
<p>First value in a tuple are positional args passed to <code>fetch_ohlcv</code> function</p>
</li>
<li>Second value in a tuple are keyword args passed to <code>fetch_ohlcv</code> function</li>
<li>
<p>As an alternative, <code>fetch_ohlcv_mock.call_args.args</code> and
    <code>fetch_ohlcv_mock.call_args.kwargs</code> can be called for separate results of
    args/kwargs</p>
<p><code>python
self.assertEqual(fetch_ohlcv_mock.call_count, 3)
actual_args = str(fetch_ohlcv_mock.call_args_list)
expected_args = r"""
[call('BTC/USDT', since=1645660800000, bar_per_iteration=500),
call('BTC/USDT', since=1645690800000, bar_per_iteration=500),
call('BTC/USDT', since=1645720800000, bar_per_iteration=500)]
"""
self.assert_equal(actual_args, expected_args, fuzzy_match=True)</code></p>
</li>
<li>
<p>In sample above, that is continuation of previous sample,
  <code>fetch_ohlcv_mock.call_args_list</code> is called that returns all calls to mocked
  function regardless of how many times it is called</p>
</li>
<li>Useful for verifying that args passed are changing as expected</li>
</ul>
<h2 id="mock-common-external-calls-in-hunitesttestcase-class">Mock common external calls in <code>hunitest.TestCase</code> class</h2>
<pre><code class="language-python">class TestCcxtExtractor1(hunitest.TestCase):
    # Mock calls to external providers.
    get_secret_patch = umock.patch.object(ivcdexex.hsecret, &quot;get_secret&quot;)
    ccxt_patch = umock.patch.object(ivcdexex, &quot;ccxt&quot;, spec=ivcdexex.ccxt)

    def setUp(self) -&gt; None:
        super().setUp()
        #
        self.get_secret_mock: umock.MagicMock = self.get_secret_patch.start()
        self.ccxt_mock: umock.MagicMock = self.ccxt_patch.start()
        # Set dummy credentials for all tests.
        self.get_secret_mock.return_value = {&quot;apiKey&quot;: &quot;test&quot;, &quot;secret&quot;: &quot;test&quot;}

    def tearDown(self) -&gt; None:
        self.get_secret_patch.stop()
        self.ccxt_patch.stop()
        # Deallocate in reverse order to avoid race conditions.
        super().tearDown()
</code></pre>
<ul>
<li>For every unit test we want to isolate external calls and replace them with
  mocks</li>
<li>This way tests are much faster and not influenced by external factors we can
    not control</li>
<li>Mocking them in <code>setUp</code> will make other tests using this class simpler and
    ready out of the box</li>
<li>In current sample we are mocking AWS secrets and <code>ccxt</code> library</li>
<li><code>umock.patch.object</code> is creating <code>patch</code> object that is not yet activated<ul>
<li><code>patch.start()/stop()</code> is activating/deactivating patch for each test run
  in <code>setUp/tearDown</code></li>
<li><code>patch.start()</code> is returning a standard <code>MagicMock</code> object we can use to
  check various states as mentioned in previous examples and control return
  values</li>
<li>call_args, call_count, return_value, side_effect, etc.</li>
</ul>
</li>
<li>Note: Although patch initialization in static variables belongs to <code>setUp</code>,
  when this code is moved there patch is created for each test separately. We
  want to avoid that and only start/stop same patch for each test.</li>
</ul>
<h2 id="mocks-with-specs">Mocks with specs</h2>
<pre><code class="language-python"># Regular mock and external library `ccxt` is replaced with `MagicMock`
@umock.patch.object(ivcdexex, &quot;ccxt&quot;)
# Only `ccxt` is spec'd, not actual components that are &quot;deeper&quot; in the `ccxt` library.
@umock.patch.object(ivcdexex, &quot;ccxt&quot;, spec=ivcdexex.ccxt)
# Everything is spec'd recursively , including returning values/instances of `ccxt`
# functions and returned values/instances of returned values/instances, etc.
@umock.patch.object(ivcdexex, &quot;ccxt&quot;, autospec=True)
</code></pre>
<ul>
<li>First mock is not tied to any spec and we can call any attribute/function
  against the mock and the call will be memorized for inspection and the return
  value is new <code>MagicMock</code>.</li>
<li><code>ccxt_mock.test(123)</code> returns new <code>MagicMock</code> and raises no error</li>
<li>In second mock <code>ccxt.test(123)</code> would fail as such function does not exists</li>
<li>We can only call valid exchange such as <code>ccxt_mock.binance()</code> that will
    return <code>MagicMock</code>, as exchange is not part of the spec</li>
<li>In third mock everything needs to be properly called</li>
<li><code>ccxt_mock.binance()</code> will return <code>MagicMock</code> with <code>ccxt.Exchange</code> spec_id
    (in mock instance as meta)<ul>
<li>As newly <code>exchange</code> instance is with spec, we can only call real
  functions/attributes of <code>ccxt.Exchange</code> class</li>
</ul>
</li>
</ul>
<h2 id="caveats">Caveats</h2>
<pre><code class="language-python"># `datetime.now` cannot be patched directly, as it is a built-in method.
# Error: &quot;can't set attributes of built-in/extension type 'datetime.datetime'&quot;
datetime_patch = umock.patch.object(imvcdeexut, &quot;datetime&quot;, spec=imvcdeexut.datetime)
</code></pre>
<ul>
<li>Python built-in methods can not be patched</li>
</ul>
<p><code>python
  class TestExtractor1(hunitest.TestCase):
      # Mock `Extractor`'s abstract functions.
      abstract_methods_patch = umock.patch.object(
          imvcdexex.Extractor, "__abstractmethods__", new=set()
      )
      ohlcv_patch = umock.patch.object(
          imvcdexex.Extractor,
          "_download_ohlcv",
          spec=imvcdexex.Extractor._download_ohlcv,
      )</code></p>
<ul>
<li>Patching <code>__abstractmethods__</code> function of an abstract class enables us to
  instantiate and test abstract class as any regular class</li>
</ul></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../js/jquery-3.6.0.min.js"></script>
        <script src="../js/bootstrap.min.js"></script>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js"></script>
        <script src="../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
